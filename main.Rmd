---
title: "R Notebook"
output: html_notebook
---

#ML Course Opencampus SoSe 2023

This program includes the data read-in, to the data preparation, data imputation and visualization. It then continues on to test out initial linear regression models before heading on over to the implementation of a neural network. { DE or EN ?}

## Datenaufbereitung und Visualisierung
We have to install the Python environment in order to run the following pieces of code.
```{r include=FALSE}

if (!requireNamespace("reticulate", quietly = TRUE)) {
  install.packages("reticulate")
}

# Create list with required libraries
libraries <- c("dplyr", "readr", "reticulate", "ggplot2", "Metrics", "tidyverse", "glmnet", "e1071", "Metrics")


# Loop through each library and check if it's installed, if not, install it and then load it to include in this project
for (lib in libraries)
{
  if (!require(lib, character.only = TRUE))
  {
    install.packages(lib)
    library(lib, character.only = TRUE)
  }
}

# Install the reticulate package if you haven't already
if (!requireNamespace("reticulate", quietly = TRUE)) {
  install.packages("reticulate")
}

# Load the reticulate package
library(reticulate)

# Custom function to check if a Conda environment exists
condaenv_exists <- function(env_name) {
  conda_envs <- conda_list()
  return(env_name %in% conda_envs$name)
}

# Check if conda is installed, if not, install miniconda
conda_envs <- conda_list()
if (length(conda_envs) == 0) {
  install_miniconda()
}

# Create a specific Python environment if it doesn't exist
if (!condaenv_exists("r-reticulate")) {
  conda_create("r-reticulate", python_version = "3.8")
}

# Get the list of installed packages in the created environment
# conda_envs <- conda_list()
# r_reticulate_env <- conda_envs[conda_envs$name == "r-reticulate", ]
# installed_packages <- r_reticulate_env$packages

# Install required packages in the created environment
# required_packages <- c("pandas", "numpy", "tensorflow", "h5py")
#
# for (pkg in required_packages) {
#   if (!(pkg %in% installed_packages)) {
#     conda_install("r-reticulate", pkg)
#   }
# }

# If this doesn't install 'tensorflow', go ahead and download anaconda from "https://www.anaconda.com/", open it up, go to environment and choose "r-reticulate". Then install the packages " "pandas", "numpy", "tensorflow", "h5py" " from the UI.
# Error associated with this fix:
# 'InvalidArchiveError("Error with archive C:\\Users\\ [...] \\\compose_set_interface.h.inc'")'
```

### Data Import

```{r include=FALSE}
# Storing Data sets as data frames

revenue <- read.csv('./data/original/umsatzdaten_gekuerzt.csv')
kiwo <- read.csv('./data/original/kiwo.csv')
ferien <- read.csv('./data/original/schulferienSH.csv')
weather <- read.csv('./data/original/weather.csv')

```

### Joining the kiwo data with the revenue data

Replacing all instances of Kiwo = NA with Kiwo = 0 Reformating Column KielerWoche to boolean values

```{r include=FALSE}
df <- left_join(revenue, kiwo, by = "Datum") %>%
  replace_na(list(KielerWoche = 0)) %>%
  mutate(KielerWoche = ifelse(KielerWoche == 1, TRUE, FALSE))

```

### Joining the weather data with the df containing revenue and kiwo

```{r include=FALSE}
df <- left_join(df, weather, by = "Datum")

```

### Cleaning up the revenue data to convert Datum column to correct data type (i.e. char -\> date)

```{r include=FALSE}
df$Datum <- as.Date(df$Datum, format = "%Y-%m-%d")
```

### Converting the "Warengruppe" to char, as the algorithm might interpret the numeric values as ordered, while they are categorial.

```{r include=FALSE}
df <- df %>%
  mutate(Warengruppe = ifelse(Warengruppe == 1, "Brot", Warengruppe)) %>%
  mutate(Warengruppe = ifelse(Warengruppe == 2, "Brötchen", Warengruppe)) %>%
  mutate(Warengruppe = ifelse(Warengruppe == 3, "Croissant", Warengruppe)) %>%
  mutate(Warengruppe = ifelse(Warengruppe == 4, "Konditorei", Warengruppe)) %>%
  mutate(Warengruppe = ifelse(Warengruppe == 5, "Kuchen", Warengruppe)) %>%
  mutate(Warengruppe = ifelse(Warengruppe == 6, "Saisonbrot", Warengruppe))
```

### Adding weekday column to data set

```{r include=FALSE}
df$Wochentag <- weekdays(df$Datum)

```

### Adding holiday column to data set

```{r include=FALSE}
ferien$Datum <- as.Date(ferien$Datum, format = "%Y-%m-%d")

df <- left_join(df, ferien, by = "Datum") %>%
  replace_na(list(FerienSH = 0)) %>%
  mutate(FerienSH = ifelse(FerienSH == 1, TRUE, FALSE))

```

### Adding Month variable
```{r}


```


### This helps us figure out if we have NA values in the dataset

```{r include=FALSE}
any(is.na(df))

any(is.na(df$Datum))
any(is.na(df$Warengruppe))
any(is.na(df$Umsatz))
any(is.na(df$KielerWoche))
any(is.na(df$Wochentag))
any(is.na(df$FerienSH))

any(is.na(df$Bewoelkung))
any(is.na(df$Temperatur))
any(is.na(df$Windgeschwindigkeit))
any(is.na(df$Wettercode))

```

### Deleting all rows that contain even one NA value (This section will be replaced with data imputation later on.)

```{r include=FALSE}
# Check for complete cases
complete_rows <- complete.cases(df)

# Subset the data frame to keep only complete rows
clean_df <- df[complete_rows, ]

#From here on out I'll work with the clean_df data set. Once the imputation step is implemented, the following code can be used for both.
```

## Prediction Models (LR, SVM, NN)

Teilen des Datensatzes in Trainings- Validierungs- und Testdatensatz. Diese werden für LR und SVM verwendet.

```{r include=FALSE}
# Set a random seed for reproducibility
set.seed(42)

# Shuffle the data
data_shuffled <- clean_df %>% 
  sample_frac(1)

# Calculate the number of rows for each dataset
n_total <- nrow(clean_df)
n_train <- floor(0.7 * n_total)
n_validation <- floor(0.20 * n_total)

# Split the data into training, validation, and test datasets
train_data <- data_shuffled %>%
  slice(1:n_train)

validation_data <- data_shuffled %>%
  slice((n_train + 1):(n_train + n_validation))

test_data <- data_shuffled %>%
  slice((n_train + n_validation + 1):n_total)

# Check the dimensions of the datasets
cat("Training dataset dimensions:", dim(train_data), "\n")
cat("Validation dataset dimensions:", dim(validation_data), "\n")
cat("Test dataset dimensions:", dim(test_data), "\n")

```

### LR Beispiel einer einfachen linearen Regression

```{r include=FALSE}
  mod_lr <- lm(Umsatz ~ Datum, clean_df)
    summary(mod_lr)
    
  mod_lr_1 <- lm(Umsatz ~ Datum + as.factor(Warengruppe), clean_df)
    summary(mod_lr_1)
    
  mod_lr_2 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag), clean_df)
    summary(mod_lr_2)
    
  mod_lr_3 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH, clean_df)
    summary(mod_lr_3)
    
  mod_lr_4 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur, clean_df)
    summary(mod_lr_4)
  
  mod_lr_5 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche, clean_df)
    summary(mod_lr_5)
  
  mod_lr_6 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung, clean_df)
    summary(mod_lr_6)
    
    # Quick comment: KiWo and Bewoelkung do not increase R-squared
    mod_lr_7 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit, clean_df)
    summary(mod_lr_7)
  
    # Windgeschwindigkeit only inproves R-squared marginally.
  mod_lr_8 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + Wettercode, clean_df)
    summary(mod_lr_8)
    
mod_lr_9 <- lm(Umsatz ~ Datum * Temperatur + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + Wettercode, clean_df)
    summary(mod_lr_9)

    # Monat als
  
  
```

#### Nutzung des resultierenden Modells für eine Vohersage

```{r include=FALSE}
# Make predictions using the test data
predicted_values <- predict(mod_lr, newdata = validation_data)

# Compare the predicted values with the actual values
comparison <- data.frame(Actual = validation_data$Umsatz, Predicted = predicted_values)

# Calculate the mean squared error (RMSE)
rmse <- sqrt(mean((comparison$Actual - comparison$Predicted)^2))

# Display the comparison and RMSE
head(comparison)
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

```

### SVM Support Vektor Maschine

Das Training kann mit Default Hyperparametern durchgeführt werden, oder gegebenenfalls mit selbst angepassten Hyperparametern. Zunächst wird ein Modell trainiert, welches die Standardparameter verwendet.

```{r}
# mod_svm_1 <- svm(Umsatz ~ Datum, train_data)
# save(mod_svm_1, file = "./ML_models/mod_svm_1")
load("./ML_models/mod_svm_1")
```

```{r}
# mod_svm_tune_1 <- tune(svm, Umsatz ~ Datum + Warengruppe + Bewoelkung + Wochentag, data=train_data, ranges = list(epsilon = seq(0.2, 1, 0.1), cost = 2^(2:3)))
#save(mod_svm_tune_1, file = "mod_svm_tune_1.RData")
load("./ML_models/mod_svm_tune_1.RData")
# mod_svm_tune_2 <- tune(svm, Umsatz ~ Warengruppe + Wochentag, data=train_data, ranges = list(epsilon = seq(0.2, 1, 0.1), cost = 2^(2:3)))
```

Chat GPT: Explanation of the tune() function and especially of the tuning parameters:
'
- 'epsilon' is a tuning parameter for the svm function. It specifies the epsilon value, which controls the tolerance for error in the SVM model. In this case, epsilon is set to a sequence of values generated using the seq() function. The seq(0.2, 1, 0.1) generates a sequence from 0.2 to 1, incrementing by 0.1 at each step.

- 'cost' is another tuning parameter for the svm function. It controls the 'cost' parameter, which determines the trade-off between achieving a low training error and allowing more flexibility in the SVM model. Here, cost is set to 2\^(2:3). The 2:3 generates the sequence 2, 3, and 2\^(2:3) calculates 2\^2 and 2\^3, resulting in the values 4 and 8, respectively."
'

Vorhersagequalität der SVM ohne angepasste Hyperparameter:
```{r}
pred_train_default <- predict(mod_svm_1, train_data)
mape(train_data$Umsatz, pred_train_default)
```

Vorhersagequalität der SVM mit angepassten Hyperparametern:
```{r}
pred_train_custom_1 <- predict(mod_svm_tune_1$best.model, train_data)
mape(train_data$Umsatz, pred_train_custom_1)

# pred_train_custom_2 <- predict(mod_svm_tune_2$best.model, train_data)
# mape(train_data$Umsatz, pred_train_custom_2)
```

Now we also have to check, whether the trained model is capable of explaining the test data set.
```{r}
pred_test_default <- predict(mod_svm_1, test_data)
cat("The Mape on default hyperparameter svm is:", mape(test_data$Umsatz, pred_test_default), "\n")

pred_test_custom_1 <- predict(mod_svm_tune_1$best.model, test_data)
cat("The Mape on custom_1 hyperparameter svm is:", mape(test_data$Umsatz, pred_test_custom_1), "\n")

# pred_test_custom_2 <- predict(mod_svm_tune_2$best.model, test_data)
# cat("The Mape on custom_2 hyperparameter svm is:", mape(test_data$Umsatz, pred_test_custom_2), "\n")
```

#### NN Neural Net

```{r}
# Use the created Python environment
use_condaenv("r-reticulate", required = TRUE)
```





