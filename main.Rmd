---
title: "R Notebook"
output: html_notebook
---

#ML Course Opencampus SoSe 2023

This program includes the data read-in, to the data preparation, data imputation and visualization. It then continues on to test out initial linear regression models before heading on over to the implementation of a neural network. { DE or EN ?}

## Datenaufbereitung und Visualisierung

We have to install the Python environment in order to run the following pieces of code.

```{r include=FALSE}

if (!requireNamespace("reticulate", quietly = TRUE)) {
  install.packages("reticulate")
}

# Create list with required libraries
libraries <- c("dplyr", "readr", "reticulate", "ggplot2", "Metrics", "tidyverse", "glmnet", "e1071", "lubridate")


# Loop through each library and check if it's installed, if not, install it and then load it to include in this project
for (lib in libraries)
{
  if (!require(lib, character.only = TRUE))
  {
    install.packages(lib)
    library(lib, character.only = TRUE)
  }
}

# Install the reticulate package if you haven't already
if (!requireNamespace("reticulate", quietly = TRUE)) {
  install.packages("reticulate")
}

# Custom function to check if a Conda environment exists
condaenv_exists <- function(env_name) {
  conda_envs <- conda_list()
  return(env_name %in% conda_envs$name)
}

# Check if conda is installed, if not, install miniconda
conda_envs <- conda_list()
if (length(conda_envs) == 0) {
  install_miniconda()
}

# Create a specific Python environment if it doesn't exist
if (!condaenv_exists("r-reticulate")) {
  conda_create("r-reticulate", python_version = "3.8")
}

# Get the list of installed packages in the created environment
# conda_envs <- conda_list()
# r_reticulate_env <- conda_envs[conda_envs$name == "r-reticulate", ]
# installed_packages <- r_reticulate_env$packages

# Install required packages in the created environment
# required_packages <- c("pandas", "numpy", "tensorflow", "h5py")
#
# for (pkg in required_packages) {
#   if (!(pkg %in% installed_packages)) {
#     conda_install("r-reticulate", pkg)
#   }
# }

# If this doesn't install 'tensorflow', go ahead and download anaconda from "https://www.anaconda.com/", open it up, go to environment and choose "r-reticulate". Then install the packages " "pandas", "numpy", "tensorflow", "h5py" " from the UI.
# Error associated with this fix:
# 'InvalidArchiveError("Error with archive C:\\Users\\ [...] \\\compose_set_interface.h.inc'")'
```

```{r}
clean_df <- read_csv("./src/data/data.csv", show_col_types = FALSE)
```

## Prediction Models (LR, SVM, NN)

Teilen des Datensatzes in Trainings- Validierungs- und Testdatensatz. Diese werden für LR und SVM verwendet.

```{r include=FALSE}
# Set a random seed for reproducibility
set.seed(42)

# Shuffle the data
data_shuffled <- clean_df %>% 
  sample_frac(1)

# Calculate the number of rows for each dataset
n_total <- nrow(clean_df)
n_train <- floor(0.7 * n_total)
n_validation <- floor(0.20 * n_total)

# Split the data into training, validation, and test datasets
train_data <- data_shuffled %>%
  slice(1:n_train)

validation_data <- data_shuffled %>%
  slice((n_train + 1):(n_train + n_validation))

test_data <- data_shuffled %>%
  slice((n_train + n_validation + 1):n_total)

# Check the dimensions of the datasets
cat("Training dataset dimensions:", dim(train_data), "\n")
cat("Validation dataset dimensions:", dim(validation_data), "\n")
cat("Test dataset dimensions:", dim(test_data), "\n")

```

### LR Beispiel einer einfachen linearen Regression

```{r include=FALSE}
  mod_lr <- lm(Umsatz ~ Datum, clean_df)
    summary(mod_lr)
    
  mod_lr_1 <- lm(Umsatz ~ Datum + as.factor(Warengruppe), clean_df)
    summary(mod_lr_1)
    
  mod_lr_2 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag), clean_df)
    summary(mod_lr_2)
    
  mod_lr_3 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH, clean_df)
    summary(mod_lr_3)
    
  mod_lr_4 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur, clean_df)
    summary(mod_lr_4)
  
  mod_lr_5 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche, clean_df)
    summary(mod_lr_5)
  
  mod_lr_6 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung, clean_df)
    summary(mod_lr_6)
    
    # Quick comment: KiWo and Bewoelkung do not increase R-squared
    mod_lr_7 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit, clean_df)
    summary(mod_lr_7)
  
    # Windgeschwindigkeit only inproves R-squared marginally.
  mod_lr_8 <- lm(Umsatz ~ Datum + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + Wettercode, clean_df)
    summary(mod_lr_8)

mod_lr_9 <- lm(Umsatz ~ Datum * Temperatur + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + Wettercode, clean_df)
    summary(mod_lr_9)
    
mod_lr_10 <- lm(Umsatz ~ Datum * Temperatur + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + as.factor(Wettercode), clean_df)
    summary(mod_lr_10)
    
mod_lr_11 <- lm(Umsatz ~ Datum * Temperatur + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + as.factor(Wettercode) + Monat, clean_df)
    summary(mod_lr_11)
    
mod_lr_12 <- lm(Umsatz ~ Datum * Temperatur + as.factor(Warengruppe) + as.factor(Wochentag) + FerienSH + Temperatur + KielerWoche + Bewoelkung + Windgeschwindigkeit + as.factor(Wettercode) + Monat + Monat * Temperatur, clean_df)
    summary(mod_lr_12)
  
  
```

#### Nutzung des resultierenden Modells für eine Vohersage

```{r include=FALSE}
# Make predictions using the test data
predicted_values <- predict(mod_lr, newdata = validation_data)

# Compare the predicted values with the actual values
comparison <- data.frame(Actual = validation_data$Umsatz, Predicted = predicted_values)

# Calculate the mean squared error (RMSE)
rmse <- sqrt(mean((comparison$Actual - comparison$Predicted)^2))

# Display the comparison and RMSE
head(comparison)
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

```

### SVM Support Vektor Maschine

Das Training kann mit Default Hyperparametern durchgeführt werden, oder gegebenenfalls mit selbst angepassten Hyperparametern. Zunächst wird ein Modell trainiert, welches die Standardparameter verwendet.

```{r}
# mod_svm_1 <- svm(Umsatz ~ Datum, train_data)
# save(mod_svm_1, file = "./ML_models/mod_svm_1.RData")
load("./ML_models/mod_svm_1.RData")
```

```{r}
# mod_svm_tune_1 <- tune(svm, Umsatz ~ Datum + Warengruppe + Bewoelkung + Wochentag, data=train_data, ranges = list(epsilon = seq(0.2, 1, 0.1), cost = 2^(2:3)))
#save(mod_svm_tune_1, file = "mod_svm_tune_1.RData")
load("./ML_models/mod_svm_tune_1.RData")
# mod_svm_tune_2 <- tune(svm, Umsatz ~ Warengruppe + Wochentag, data=train_data, ranges = list(epsilon = seq(0.2, 1, 0.1), cost = 2^(2:3)))
```

Chat GPT: Explanation of the tune() function and especially of the tuning parameters: ' - 'epsilon' is a tuning parameter for the svm function. It specifies the epsilon value, which controls the tolerance for error in the SVM model. In this case, epsilon is set to a sequence of values generated using the seq() function. The seq(0.2, 1, 0.1) generates a sequence from 0.2 to 1, incrementing by 0.1 at each step.

-   'cost' is another tuning parameter for the svm function. It controls the 'cost' parameter, which determines the trade-off between achieving a low training error and allowing more flexibility in the SVM model. Here, cost is set to 2\^(2:3). The 2:3 generates the sequence 2, 3, and 2\^(2:3) calculates 2\^2 and 2\^3, resulting in the values 4 and 8, respectively." '

Vorhersagequalität der SVM ohne angepasste Hyperparameter:

```{r}
pred_train_default <- predict(mod_svm_1, train_data)
mape(train_data$Umsatz, pred_train_default)
```

Vorhersagequalität der SVM mit angepassten Hyperparametern:

```{r}
pred_train_custom_1 <- predict(mod_svm_tune_1$best.model, train_data)
mape(train_data$Umsatz, pred_train_custom_1)

# pred_train_custom_2 <- predict(mod_svm_tune_2$best.model, train_data)
# mape(train_data$Umsatz, pred_train_custom_2)
```

Now we also have to check, whether the trained model is capable of explaining the test data set.

```{r}
pred_test_default <- predict(mod_svm_1, test_data)
cat("The Mape on default hyperparameter svm is:", mape(test_data$Umsatz, pred_test_default), "\n")

pred_test_custom_1 <- predict(mod_svm_tune_1$best.model, test_data)
cat("The Mape on custom_1 hyperparameter svm is:", mape(test_data$Umsatz, pred_test_custom_1), "\n")

# pred_test_custom_2 <- predict(mod_svm_tune_2$best.model, test_data)
# cat("The Mape on custom_2 hyperparameter svm is:", mape(test_data$Umsatz, pred_test_custom_2), "\n")
```

#### NN Neural Net

```{r}
# Use the created Python environment
use_condaenv("r-reticulate", required = TRUE)

```

Preparing data once more in a different configuration

```{r}
# Preparation of independent variables ('features') by dummy coding the categorical variables
features <- as_tibble(model.matrix(Umsatz ~ as.factor(Warengruppe) + as.factor(Wettercode) +  as.factor(Wochentag) + as.factor(Monat) + Datum + Bewoelkung + Temperatur + KielerWoche + Windgeschwindigkeit, clean_df))
names(features)

# Construction of prepared data set 
prepared_data <- tibble(label=clean_df$Umsatz, features)  
# inclusion of the dependent variable ('label')

```

Now since this is a different data type as the previously used one for the LR and SVM, we need to create the basic, training and validation data sets again:

```{r}
set.seed(42)
prepared_data_shuffled <- prepared_data %>%
  sample_frac(1)

n_total_NN <- nrow(prepared_data_shuffled)
n_training_NN <- floor(0.7 * n_total_NN)
n_validation_NN <- floor(0.2 * n_total_NN)

```

Here we need to split the features and labels for training, validation and test data sets.

```{r}
training_features <-
  prepared_data_shuffled %>% select(-label) %>% slice(1:n_training_NN)

validation_features <-
  prepared_data_shuffled %>% select(-label) %>% slice((n_training_NN + 1):(n_training_NN + n_validation_NN))

test_features <-
  prepared_data_shuffled %>% select(-label) %>% slice((n_training_NN + n_validation_NN + 1):n_total_NN)

training_label <-
  prepared_data_shuffled %>% select(label) %>% slice(1:n_training_NN)

validation_label <-
  prepared_data_shuffled %>% select(label) %>% slice((n_training_NN + 1):(n_training_NN + n_validation_NN))

test_label <-
  prepared_data_shuffled %>% select(label) %>% slice((n_training_NN + n_validation_NN + 1):n_total_NN)

```

```{r}
# Check the dimensions of the dataframes
cat("Training features dimensions:", dim(training_features), "\n")
cat("Validation features dimensions:",
    dim(validation_features),
    "\n")
cat("Test features dimensions:", dim(test_features), "\n")
cat("\n")
cat("Training label dimensions:", dim(training_label), "\n")
cat("Validation label dimensions:", dim(validation_label), "\n")
cat("Test label dimensions:", dim(test_label), "\n")
```

Now let's go on to train the Neural Network! \### Definition des Neuronalen Netzes

```{python}
# Import needed Python libraries and functions
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam

# The argument "input_shape" for the definition of the input layer must include the number input variables (features) used for the model. To automatically calculate this number, we use the  function `r.training_features.keys()`, which returns the list of variable names of the dataframe `training_features`. Then, the function `len()` returns the length of this list of variable names (i.e. the number of variables in the input).

model = Sequential([
  InputLayer(input_shape=(len(r.training_features.keys()), )),
  BatchNormalization(),
  Dense(10, activation='relu'),
  Dense(4, activation='relu'),
  Dense(1)
])

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()


```

### Schätzung des neuronalen Netzes

```{python}

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse", optimizer=Adam(learning_rate=0.01))

# Schaetzung des Modells
history = model.fit(r.training_features, r.training_label, epochs=1000,
                    validation_data = (r.validation_features, r.validation_label), verbose=0)

# Ggf. Speichern des geschaetzten Modells
model.save("./ML_models/mod_nn_1.h5")

```

### Auswertung der Modelloptimierung

```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))

# Plot
ggplot(data[-(1:10),]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 


```

### Auswertung der Schätzergebnisse

```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
training_predictions <- py$model$predict(training_features)
validation_predictions <- py$model$predict(validation_features)
test_predictions <- py$model$predict(test_features)



# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(training_label[[1]], training_predictions), digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Validation Data:\t", format(mape(validation_label[[1]], validation_predictions), digits=3, nsmall=2)))


```

```{r}

## Grafischer Vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten

# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = training_predictions/1000, actual = training_label[[1]]/1000)
data_test <- data.frame(prediction = validation_predictions/1000, actual = validation_label[[1]]/1000)

# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Umsatz") 

# Plot der Ergebnisse der Validierungsdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Umsatz") 


```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorhergesagter Umsatz:\t", round(validation_predictions[100])))
cat(paste0("\nTatsächlicher Umsatz:\t", validation_label[[1]][100]))


```
